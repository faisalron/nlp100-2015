{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface: はてな, base: はてな, pos: 感動詞, pos1: *\n",
      "surface: 何, base: 何, pos: 名詞, pos1: 代名詞\n",
      "surface: でも, base: でも, pos: 助詞, pos1: 副助詞\n",
      "surface: 容子, base: 容子, pos: 名詞, pos1: 一般\n",
      "surface: が, base: が, pos: 助詞, pos1: 格助詞\n",
      "surface: おかしい, base: おかしい, pos: 形容詞, pos1: 自立\n",
      "surface: と, base: と, pos: 助詞, pos1: 接続助詞\n",
      "surface: 、, base: 、, pos: 記号, pos1: 読点\n",
      "surface: のそのそ, base: のそのそ, pos: 副詞, pos1: 一般\n",
      "surface: 這い出し, base: 這い出す, pos: 動詞, pos1: 自立\n",
      "surface: て, base: て, pos: 助詞, pos1: 接続助詞\n",
      "surface: 見る, base: 見る, pos: 動詞, pos1: 自立\n",
      "surface: と, base: と, pos: 助詞, pos1: 接続助詞\n",
      "surface: 非常, base: 非常, pos: 名詞, pos1: 形容動詞語幹\n",
      "surface: に, base: に, pos: 助詞, pos1: 副詞化\n",
      "surface: 痛い, base: 痛い, pos: 形容詞, pos1: 自立\n",
      "surface: 。, base: 。, pos: 記号, pos1: 句点\n"
     ]
    }
   ],
   "source": [
    "# 40. 係り受け解析結果の読み込み（形態素）\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "    def __str__(self):\n",
    "        return \"surface: {}, base: {}, pos: {}, pos1: {}\".format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "sentence_list = []\n",
    "sentence = []\n",
    "with open(\"./neko.txt.cabocha\", \"r\", encoding=\"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        if line == \"EOS\\n\": #End Of Sentence\n",
    "            sentence_list.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "        elif line.split()[0] == \"*\":\n",
    "            continue\n",
    "        surface, features = line.split(\"\\t\")\n",
    "        morph = Morph(surface=surface, base=features.split(\",\")[6], pos=features.split(\",\")[0], pos1=features.split(\",\")[1])\n",
    "        sentence.append(morph)\n",
    "        \n",
    "for morph in sentence_list[32]:\n",
    "    print(str(morph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文節番号 : 0, 文字列: この, 係り先文節番号 1\n",
      "文節番号 : 1, 文字列: 書生というのは, 係り先文節番号 7\n",
      "文節番号 : 2, 文字列: 時々, 係り先文節番号 4\n",
      "文節番号 : 3, 文字列: 我々を, 係り先文節番号 4\n",
      "文節番号 : 4, 文字列: 捕えて, 係り先文節番号 5\n",
      "文節番号 : 5, 文字列: 煮て, 係り先文節番号 6\n",
      "文節番号 : 6, 文字列: 食うという, 係り先文節番号 7\n",
      "文節番号 : 7, 文字列: 話である。, 係り先文節番号 -1\n"
     ]
    }
   ],
   "source": [
    "# 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "class Chunk(object):\n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.dst = int(dst.strip(\"D\"))\n",
    "        self.srcs = int(srcs)\n",
    "        \n",
    "    def get_chunk_text(self):\n",
    "        text = \"\"\n",
    "        for morph in self.morphs:\n",
    "            if not morph.pos == \"記号\":\n",
    "                text += morph.surface\n",
    "        return text\n",
    "    \n",
    "    def has_noun(self):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == \"名詞\":\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def has_verb(self):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == \"動詞\":\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def has_particle(self):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == \"助詞\":\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def is_sahen_wo(self):\n",
    "        return len(self.morphs) == 2 and self.morphs[0].pos1 == \"サ変接続\" and self.morphs[1].base == \"を\"\n",
    "    \n",
    "    def get_last_particle(self):\n",
    "        return [_morph for _morph in self.morphs if _morph.pos == '助詞'][-1] if self.has_particle() else None\n",
    "    \n",
    "    def get_first_verb(self):\n",
    "        return [_morph for _morph in self.morphs if _morph.pos == '動詞'][0] if self.has_verb() else None\n",
    "    \n",
    "    \n",
    "    def replace_noun(self, alt):\n",
    "        for _morph in self.morphs:\n",
    "            if _morph.pos == '名詞':\n",
    "                _morph.surface = alt\n",
    "\n",
    "\n",
    "sentence_list = []\n",
    "sentence = []\n",
    "chunk = None\n",
    "with open(\"./neko.txt.cabocha\", \"r\", encoding=\"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        line_list = line.split()\n",
    "        if line_list[0] == \"*\":\n",
    "            if chunk is not None:\n",
    "                sentence.append(chunk)\n",
    "            chunk = Chunk(morphs=[], dst=line_list[2], srcs=line_list[1])\n",
    "        elif line_list[0] == \"EOS\":\n",
    "            if chunk is not None:\n",
    "                sentence.append(chunk)\n",
    "            if len(sentence) > 0:\n",
    "                sentence_list.append(sentence)\n",
    "            sentence = []\n",
    "            chunk = None\n",
    "            continue\n",
    "        else:\n",
    "            surface, features = line.split(\"\\t\")\n",
    "            morph = Morph(surface=surface, base=features.split(\",\")[6], pos=features.split(\",\")[0], pos1=features.split(\",\")[1])\n",
    "            chunk.morphs.append(morph)\n",
    "            \n",
    "for chunk in sentence_list[7]:\n",
    "    text = \"\"\n",
    "    for morph in chunk.morphs:\n",
    "        text += morph.surface\n",
    "    print(\"文節番号 : \" + str(chunk.srcs) + \", 文字列: \" + text + \", 係り先文節番号 \" + str(chunk.dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====文=====\n",
      "しかも\t種族であったそうだ\n",
      "あとで\t聞くと\n",
      "聞くと\t種族であったそうだ\n",
      "それは\t種族であったそうだ\n",
      "書生という\t人間中で\n",
      "人間中で\t種族であったそうだ\n",
      "一番\t獰悪な\n",
      "獰悪な\t種族であったそうだ\n",
      "=====文=====\n",
      "この\t書生というのは\n",
      "書生というのは\t話である\n",
      "時々\t捕えて\n",
      "我々を\t捕えて\n",
      "捕えて\t煮て\n",
      "煮て\t食うという\n",
      "食うという\t話である\n"
     ]
    }
   ],
   "source": [
    "# 42. 係り元と係り先の文節の表示\n",
    "for sentence in sentence_list[6:8]:\n",
    "    print(\"=====文=====\")\n",
    "    for chunk in sentence:\n",
    "        if not chunk.dst == -1:\n",
    "            print(chunk.get_chunk_text() + \"\\t\" + sentence[chunk.dst].get_chunk_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====文=====\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "ものを\t見た\n",
      "=====文=====\n",
      "あとで\t聞くと\n",
      "=====文=====\n",
      "我々を\t捕えて\n"
     ]
    }
   ],
   "source": [
    "# 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "for sentence in sentence_list[5:8]:\n",
    "    print(\"=====文=====\")\n",
    "    for chunk in sentence:\n",
    "        if not chunk.dst == -1 and chunk.has_noun() and sentence[chunk.dst].has_verb():\n",
    "            print(chunk.get_chunk_text() + \"\\t\" + sentence[chunk.dst].get_chunk_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "\"dot.exe\" not found in path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32mc:\\python34\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format)\u001b[0m\n\u001b[1;32m   1870\u001b[0m                 \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1871\u001b[0;31m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0m\u001b[1;32m   1872\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\python34\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[1;32m    858\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\python34\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[1;32m   1113\u001b[0m                                          \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m                                          startupinfo)\n\u001b[0m\u001b[1;32m   1115\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [WinError 2] 指定されたファイルが見つかりません。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f3a8b03c7b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msentence_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"digraph sentence{\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_dot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_jpeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./jpg/sentence{}.jpg\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mc:\\python34\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path, f, prog)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                 \u001b[0;34m'write_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfrmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                     self.write(path, format=f, prog=prog))\n\u001b[0m\u001b[1;32m   1686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'write_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfrmt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\python34\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, path, prog, format)\u001b[0m\n\u001b[1;32m   1766\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\python34\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                 raise Exception(\n\u001b[1;32m   1875\u001b[0m                     '\"{prog}\" not found in path.'.format(\n\u001b[0;32m-> 1876\u001b[0;31m                         prog=prog))\n\u001b[0m\u001b[1;32m   1877\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: \"dot.exe\" not found in path."
     ]
    }
   ],
   "source": [
    "# 44. 係り受け木の可視化\n",
    "import pydot\n",
    "for idx, sentence in enumerate(sentence_list[5:8]):\n",
    "    nodes = ['\"{}\"->\"{}\"; '.format(chunk.get_chunk_text(), sentence[chunk.dst].get_chunk_text()) for chunk in sentence if  not chunk.dst == -1]\n",
    "    sentence_dot = \"digraph sentence{\" + \"\".join(nodes) + \"}\"\n",
    "    g = pydot.graph_from_dot_data(sentence_dot)\n",
    "    g[0].write_jpeg(\"./jpg/sentence{}.jpg\".format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 45. 動詞の格パターンの抽出\n",
    "# (\"見る\", [\"は\", \"を\"])というタプルのリスト\n",
    "case_pattern_list = []\n",
    "for sentence in sentence_list:\n",
    "    for chunk in sentence:\n",
    "        if not chunk.has_verb():\n",
    "            continue\n",
    "        particles = [ch.get_last_particle().base for ch in sentence if ch.dst == chunk.srcs and ch.has_particle()]\n",
    "        if len(particles) == 0:\n",
    "            continue\n",
    "        particles.sort()\n",
    "        case_pattern = (chunk.get_first_verb().base, particles)\n",
    "        case_pattern_list.append(case_pattern)\n",
    "        \n",
    "with open(\"./case_patterns.txt\", \"w\") as f:\n",
    "    for case_pattern in case_pattern_list:\n",
    "        f.write(\"{}\\t{}\\n\".format(case_pattern[0], \" \".join(case_pattern[1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "始める\tで\tここで\n",
      "見る\tは を\t吾輩は ものを\n",
      "聞く\tで\tあとで\n",
      "捕える\tを\t我々を\n",
      "煮る\tて\t捕えて\n",
      "食う\tて\t煮て\n"
     ]
    }
   ],
   "source": [
    "# 46. 動詞の格フレーム情報の抽出\n",
    "# (\"見る\", [\"は\", \"を\"], [\"吾輩は\", \"ものを\"])というタプルのリスト\n",
    "case_frame_pattern_list = []\n",
    "for sentence in sentence_list[5:8]:\n",
    "    for chunk in sentence:\n",
    "        if not chunk.has_verb():\n",
    "            continue\n",
    "        particles = [(ch.get_last_particle().base, ch.get_chunk_text()) for ch in sentence if ch.dst == chunk.srcs and ch.has_particle()]\n",
    "        if len(particles) == 0:\n",
    "            continue\n",
    "        sorted_particles = sorted(particles, key=lambda x:x[0])\n",
    "        particle_list, chunk_text_list = [], []\n",
    "        for t in sorted_particles:\n",
    "            particle_list.append(t[0])\n",
    "            chunk_text_list.append(t[1])\n",
    "        case_pattern = (chunk.get_first_verb().base, particle_list, chunk_text_list)\n",
    "        case_frame_pattern_list.append(case_pattern)\n",
    "        \n",
    "for cfp in case_frame_pattern_list:\n",
    "    print(cfp[0] + \"\\t\" + \" \".join(cfp[1]) + \"\\t\" + \" \".join(cfp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "研究を続ける\tて て\t組織しまして 会合して\n",
      "同情を持つ\tに\t人物に\n",
      "質問をする\tが て\tしたが 見えて\n",
      "御馳走を致す\tに\t東風子に\n",
      "返事をする\tと に は\t及ばんさと 手紙に 主人は\n",
      "著述を居る\tて より\t依て 此間中より\n",
      "厚遇を受ける\tが まで\t猫が かくまで\n",
      "返事をする\tは\t下女は\n",
      "含嗽をやる\tで\t毎朝風呂場で\n"
     ]
    }
   ],
   "source": [
    "# 47. 機能動詞構文のマイニング\n",
    "sahen_frame_pattern_list = []\n",
    "for sentence in sentence_list[800:1000]:\n",
    "    for chunk in sentence:\n",
    "        if not chunk.has_verb():\n",
    "            continue\n",
    "        particles = [(ch.get_last_particle().base, ch) for ch in sentence if ch.dst == chunk.srcs and ch.has_particle()]\n",
    "        if len(particles) == 1:\n",
    "            continue\n",
    "        # 動詞に「サ変接続+を」が係るかチェック\n",
    "        has_sahen_wo = False\n",
    "        for particle in particles:\n",
    "            if  particle[1].is_sahen_wo():\n",
    "                has_sahen_wo = True\n",
    "                sahen_wo = particle[1].get_chunk_text()\n",
    "        if not has_sahen_wo:\n",
    "            continue\n",
    "        sorted_particles = sorted(particles, key=lambda x:x[0])\n",
    "        particle_list, chunk_text_list = [], []\n",
    "        for t in sorted_particles:\n",
    "            if t[1].get_chunk_text() == sahen_wo:\n",
    "                continue\n",
    "            particle_list.append(t[0])\n",
    "            chunk_text_list.append(t[1].get_chunk_text())\n",
    "        sahen_pattern = (sahen_wo + chunk.get_first_verb().base, particle_list, chunk_text_list)\n",
    "        sahen_frame_pattern_list.append(sahen_pattern)\n",
    "        \n",
    "for cfp in sahen_frame_pattern_list:\n",
    "    print(cfp[0] + \"\\t\" + \" \".join(cfp[1]) + \"\\t\" + \" \".join(cfp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は -> 見た\n",
      "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
      "人間という -> ものを -> 見た\n",
      "ものを -> 見た\n",
      "あとで -> 聞くと -> 種族であったそうだ\n",
      "それは -> 種族であったそうだ\n",
      "書生という -> 人間中で -> 種族であったそうだ\n",
      "人間中で -> 種族であったそうだ\n",
      "一番 -> 獰悪な -> 種族であったそうだ\n",
      "獰悪な -> 種族であったそうだ\n",
      "書生というのは -> 話である\n",
      "我々を -> 捕えて -> 煮て -> 食うという -> 話である\n"
     ]
    }
   ],
   "source": [
    "# 48. 名詞から根へのパスの抽出\n",
    "def get_path_list(sentence, idx, paths):\n",
    "    if sentence[idx].dst != -1:\n",
    "        paths.append(sentence[idx])\n",
    "        get_path_list(sentence, sentence[idx].dst, paths)\n",
    "    else:\n",
    "        paths.append(sentence[idx])\n",
    "    return paths\n",
    "\n",
    "\n",
    "paths_list = []\n",
    "for sentence in sentence_list[5:8]:\n",
    "    for chunk in sentence:\n",
    "        if chunk.has_noun():\n",
    "            paths = get_path_list(sentence, chunk.srcs, [])\n",
    "            if len(paths) == 1:\n",
    "                continue\n",
    "            paths_list.append(paths)\n",
    "            \n",
    "for paths in paths_list:\n",
    "    print(\" -> \".join([ch.get_chunk_text() for ch in paths]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは -> 見た | Yで -> 始めて -> 人間という -> ものを -> 見た | 見た\n",
      "Xは -> 見た | Yという -> ものを -> 見た | 見た\n",
      "Xは -> 見た | Yを -> 見た | 見た\n",
      "Xで -> 始めて -> Yという\n",
      "Xで -> 始めて -> Yという -> Yを\n",
      "Xという -> Yを\n",
      "Xで -> 聞くと -> 種族であったそうだ | Yは -> 種族であったそうだ | 種族であったそうだ\n",
      "Xで -> 聞くと -> 種族であったそうだ | Yという -> 人間中で -> 種族であったそうだ | 種族であったそうだ\n",
      "Xで -> 聞くと -> 種族であったそうだ | YYで -> 種族であったそうだ | 種族であったそうだ\n",
      "Xで -> 聞くと -> 種族であったそうだ | Y -> 獰悪な -> 種族であったそうだ | 種族であったそうだ\n",
      "Xで -> 聞くと -> 種族であったそうだ | Yな -> 種族であったそうだ | 種族であったそうだ\n",
      "Xで -> 聞くと -> YであったYだ\n",
      "Xは -> YであったYだ | Yという -> YYで -> YであったYだ | YであったYだ\n",
      "Xは -> YであったYだ | YYで -> YであったYだ | YであったYだ\n",
      "Xは -> YであったYだ | Y -> Yな -> YであったYだ | YであったYだ\n",
      "Xは -> YであったYだ | Yな -> YであったYだ | YであったYだ\n",
      "Xは -> YであったYだ\n",
      "Xという -> YYで\n",
      "Xという -> YYで -> YであったYだ | Y -> Yな -> YであったYだ | YであったYだ\n",
      "Xという -> YYで -> YであったYだ | Yな -> YであったYだ | YであったYだ\n",
      "Xという -> YYで -> YであったYだ\n",
      "XXで -> YであったYだ | Y -> Yな -> YであったYだ | YであったYだ\n",
      "XXで -> YであったYだ | Yな -> YであったYだ | YであったYだ\n",
      "XXで -> YであったYだ\n",
      "X -> Yな\n",
      "X -> Yな -> YであったYだ\n",
      "Xな -> YであったYだ\n",
      "XというXは -> 話である | Yを -> 捕えて -> 煮て -> 食うという -> 話である | 話である\n",
      "XというXは -> Yである\n",
      "Xを -> 捕えて -> 煮て -> 食うという -> Yである\n"
     ]
    }
   ],
   "source": [
    "# 49. 名詞間の係り受けパスの抽出\n",
    "from itertools import combinations\n",
    "\n",
    "def noun_pairs(sen):\n",
    "    noun_chunk_list = [ch for ch in sen if ch.has_noun()]\n",
    "    return list(combinations(noun_chunk_list, 2))\n",
    "\n",
    "\n",
    "def common_chunk(path_i, path_j):\n",
    "    chunk_k = None\n",
    "    path_i = list(reversed(path_i))\n",
    "    path_j = list(reversed(path_j))\n",
    "    for idx, (c_i, c_j) in enumerate(zip(path_i, path_j)):\n",
    "        if c_i.srcs != c_j.srcs:\n",
    "            chunk_k = path_i[idx - 1]\n",
    "            break\n",
    "\n",
    "    return chunk_k\n",
    "\n",
    "\n",
    "for sentence in sentence_list[5:8]:\n",
    "    n_pairs = noun_pairs(sentence)\n",
    "    if len(n_pairs) == 0:\n",
    "        continue\n",
    "    \n",
    "    for n_pair in n_pairs:\n",
    "        chunk_i, chunk_j = n_pair\n",
    "        chunk_i.replace_noun('X')\n",
    "        chunk_j.replace_noun('Y')\n",
    "        \n",
    "        paths_i = get_path_list(sentence, chunk_i.srcs, [])\n",
    "        paths_j = get_path_list(sentence, chunk_j.srcs, [])\n",
    "        \n",
    "        if chunk_j in paths_i:\n",
    "            # 文節iiから構文木の根に至る経路上に文節jjが存在する場合\n",
    "            idx_j = paths_i.index(chunk_j)\n",
    "            print(\" -> \".join([ch.get_chunk_text() for ch in paths_i[:idx_j + 1]]))\n",
    "            \n",
    "        else:\n",
    "            # 文節iiと文節jjから構文木の根に至る経路上で共通の文節kkで交わる場合\n",
    "            chunk_k = common_chunk(paths_i, paths_j)\n",
    "            if chunk_k is None:\n",
    "                continue\n",
    "            idx_k_i = paths_i.index(chunk_k)\n",
    "            idx_k_j = paths_j.index(chunk_k)\n",
    "            print(\" | \".join([\" -> \".join([ch.get_chunk_text() for ch in paths_i[:idx_k_i + 1]]),\n",
    "                             \" -> \".join([ch.get_chunk_text() for ch in paths_j[:idx_k_j + 1]]),\n",
    "                             chunk_k.get_chunk_text()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
