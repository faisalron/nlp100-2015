{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 第8章: 機械学習\n",
    "\n",
    "# 70. データの入手・整形\n",
    "import random\n",
    "\n",
    "with open('rt-polarity.neg', encoding='UTF-8', errors='ignore') as input_file:\n",
    "    negatives = ['-1 ' + line for line in input_file]\n",
    "\n",
    "with open('rt-polarity.pos', encoding='UTF-8', errors='ignore') as input_file:\n",
    "    positives = ['+1 ' + line for line in input_file]\n",
    "\n",
    "sentiments = negatives + positives\n",
    "random.shuffle(sentiments)\n",
    "\n",
    "with open('sentiment.txt', mode='w') as output_file:\n",
    "    for sentiment in sentiments: \n",
    "        output_file.write(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 71. ストップワード\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def is_stopword(word):\n",
    "    return word in stop_words\n",
    "\n",
    "print(is_stopword('they'))\n",
    "print(is_stopword('great'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14)\t2\n",
      "  (0, 43)\t4\n",
      "  (0, 7776)\t1\n",
      "  (0, 23)\t3\n",
      "  (0, 10299)\t1\n",
      "  (0, 12166)\t1\n",
      "  (0, 6219)\t1\n",
      "  (0, 10316)\t1\n",
      "  (0, 2119)\t1\n",
      "  (0, 13367)\t1\n",
      "  (0, 5964)\t1\n",
      "  (0, 10486)\t1\n",
      "  (0, 2570)\t1\n",
      "  (0, 13909)\t1\n",
      "  (0, 5452)\t1\n",
      "  (0, 2040)\t1\n",
      "  (0, 1667)\t1\n",
      "  (0, 13605)\t1\n",
      "  (0, 7504)\t1\n",
      "  (0, 6066)\t1\n",
      "  (1, 43)\t1\n",
      "  (1, 8407)\t1\n",
      "  (1, 1641)\t1\n",
      "  (1, 11470)\t1\n",
      "  (1, 2793)\t1\n",
      "  :\t:\n",
      "  (10660, 13739)\t1\n",
      "  (10660, 10024)\t1\n",
      "  (10660, 7477)\t1\n",
      "  (10660, 8514)\t1\n",
      "  (10660, 2018)\t1\n",
      "  (10660, 12648)\t1\n",
      "  (10660, 25)\t1\n",
      "  (10660, 14507)\t1\n",
      "  (10660, 2354)\t1\n",
      "  (10660, 6091)\t1\n",
      "  (10660, 1557)\t1\n",
      "  (10660, 8058)\t1\n",
      "  (10660, 1978)\t1\n",
      "  (10660, 2423)\t1\n",
      "  (10660, 6350)\t1\n",
      "  (10660, 2957)\t1\n",
      "  (10660, 3088)\t1\n",
      "  (10660, 10533)\t1\n",
      "  (10660, 13139)\t1\n",
      "  (10660, 3910)\t1\n",
      "  (10661, 43)\t1\n",
      "  (10661, 13376)\t1\n",
      "  (10661, 9707)\t1\n",
      "  (10661, 4302)\t1\n",
      "  (10661, 3601)\t1\n"
     ]
    }
   ],
   "source": [
    "# 72. 素性抽出\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "corpus = [sentiment[3:].replace('\\n', '') for sentiment in sentiments]\n",
    "label = [int(sentiment[0:2]) for sentiment in sentiments]\n",
    "\n",
    "def preprocess(doc):\n",
    "    return (stem(word) for word in doc.split() if not is_stopword(word))\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1, stop_words='english', analyzer= preprocess)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# 73. 学習\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y = label\n",
    "\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "print(logreg.fit(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "# 74. 予測\n",
    "\n",
    "# 負例\n",
    "print(\n",
    "    logreg.predict(\n",
    "        vectorizer.transform(\n",
    "            [\"' . . . mafia , rap stars and hood rats butt their ugly heads in a regurgitation of cinematic violence that gives brutal birth to an unlikely , but likable , hero . '\"]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# 正例\n",
    "print(\n",
    "    logreg.predict(\n",
    "        vectorizer.transform(\n",
    "            ['the metaphors are provocative , but too often , the viewer is left puzzled by the mechanics of the delivery . ']\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest 10 : teamwork, tenth, todd, boost, tape, award-worthi, cozi, liber, nightclub, [jackie]\n",
      "lowest 10 : choppi, prettiest, makhmalbaf, har, flaccid, disclosur, metaphys, text, expend, limp\n"
     ]
    }
   ],
   "source": [
    "# 75. 素性の重み\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sorted_indices = np.array(logreg.coef_)[0].argsort()\n",
    "\n",
    "# 最高のトップ10\n",
    "print('highest 10 : ' + ', '.join([key for key, value in vectorizer.vocabulary_.items() if (value in sorted_indices[-10:]) ]))\n",
    "\n",
    "# 最低のトップ10\n",
    "print('lowest 10 : ' + ', '.join([key for key, value in vectorizer.vocabulary_.items() if (value in sorted_indices[:10]) ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100.0%'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{}%\".format(logreg.score(vectorizer.transform([\"I love everyone in terms of teamwork\"]), [1]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n",
      "10662\n"
     ]
    }
   ],
   "source": [
    "print(len([elm for elm in y if elm == 1]))\n",
    "\n",
    "print(len([elm for elm in y if elm == -1]))\n",
    "\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
